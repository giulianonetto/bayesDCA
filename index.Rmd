---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  dpi = 300
)
```

# bayesDCA

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

Perform Bayesian Decision Curve Analysis for clinical prediction models and diagnostic tests.

When validating a clinical prediction model, you may end up with AUC 0.79 and a slight miscalibration. How do you know if that is good enough for your model to be clinically useful? The same question can be asked if you have a diagnostic test with sensitivity of 75% and specificity of 68%, for instance. Decision Curve Analysis helps us find an answer - see [Vickers, van Calster & Steyerberg, 2019](https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-019-0064-7) for an introduction to DCA. Here, we use Bayesian methods to accurately quantify uncertainty in our decisions curves - powered by [Stan](https://mc-stan.org/).

# Installation

You can install the development version of bayesDCA from [GitHub](https://github.com/) with:

```{r eval=FALSE}
# install.packages("devtools")
devtools::install_github("giulianonetto/bayesdca")
```

# Running Bayesian DCA

You can use `bayesDCA` to evaluate predictive models as well as binary tests.

All you need is a `data.frame` with a column named `outcomes` (0 or 1) and one column for each model or test being evaluated. In the example below, the `PredModelData` includes the probability predictions from a model and the results from a binary test. The names of these columns don't matter (except for the `outcomes` column).

```{r}
library(bayesDCA)
data(PredModelData)
head(PredModelData)
```

We set `cores = 4` to speed up MCMC sampling with [Stan](https://mc-stan.org/).

```{r}
fit <- dca(PredModelData, cores = 4)
plot(fit)
```

## Comparing two decision strategies

Say you want to infer whether the predictions from the model yield a better decision strategy than the binary test -- i.e., you want to compare their decision curves. Then:

```{r}
compare_dca(fit, 
            models_or_tests = c("predictions", "binary_test"))
```

## Comparing against default stratefies

Default strategies include treating all patients and treating no patients. If the decision threshold is above the prevalence of the disease, treating no patient is better than treating all patients. If you run `compare_dca` and specify only one decision strategy, `bayesDCA` will compare this strategy against the the appropriate default for each decision threshold.

```{r}
compare_dca(fit, 
            models_or_tests = "predictions")
```
